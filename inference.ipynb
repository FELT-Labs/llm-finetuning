{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Ksg1aY7pKL"
      },
      "source": [
        "# Update libraries to latest version\n",
        "This is neccessary for using this notebook in google colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEf5z6ZY7dar"
      },
      "outputs": [],
      "source": [
        "# Fix some issue with encoding:\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda x: \"UTF-8\"\n",
        "\n",
        "!pip install -q -U datasets==2.14.0 bitsandbytes==0.41.0 einops==0.6.1 trl==0.4.7 transformers==4.31.0 accelerate==0.21.0 peft==0.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90dS7yR28NcR"
      },
      "source": [
        "# Unzip finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_30ZhbJ-LZj",
        "outputId": "ac6e279e-1b9c-4692-b186-b0a87934dedf"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: Replace this with your download command (keep the \"!\" at the beggining)\n",
        "!curl 'https://provider.feltlabs.ai//api/services/computeResult?consumerAddress=0x93E760dbFd4837983B5260eA5f5DAc3bB0d3dbCC&jobId=edb08ab014fc44dea9c3521d426e1f25&index=0&nonce=1691980988065' \\\n",
        " -H 'AuthToken: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjIxMjM5NDQyNzgsImFkZHJlc3MiOiIweDkzRTc2MGRiRmQ0ODM3OTgzQjUyNjBlQTVmNURBYzNiQjBkM2RiQ0MifQ.pQC22lSGvlcKWMSoCll0oLgDbJ5b97Dak2BTlc-R4Us' \\\n",
        " --output output.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoC7seKK7rhh",
        "outputId": "055ee2b7-1a7b-4655-e197-f5c01d41b266"
      },
      "outputs": [],
      "source": [
        "!tar xf output.tar.gz -C ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2WITf5b8I34"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajc5j2DB78B0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "output_path = \"./model\"\n",
        "config = PeftConfig.from_pretrained(output_path)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0},\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, output_path)\n",
        "\n",
        "# Load original model\n",
        "model_org = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0},\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5ojBMBj8LPW"
      },
      "source": [
        "# Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfH8xH1q8Hll"
      },
      "outputs": [],
      "source": [
        "input_text = '### Question: What can you tell me about FELT Labs? ### Response:'\n",
        "\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = model.generate(input_ids=input_ids.cuda(), do_sample=True, top_p=0.95, top_k=100, max_length=100)\n",
        "\n",
        "print(\"### PREDICTION ###\")\n",
        "print(tokenizer.decode(greedy_output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjMvcBuiSe-"
      },
      "source": [
        "# Inference on original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHF1hIKGiQRw",
        "outputId": "914476cd-5ffb-41b5-cbf1-a14df4ce94e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = model_org.generate(input_ids=input_ids.cuda(), do_sample=True, top_p=0.95, top_k=100, max_length=100)\n",
        "\n",
        "print(\"### PREDICTION ###\")\n",
        "print(tokenizer.decode(greedy_output[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
